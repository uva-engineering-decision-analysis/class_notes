[
["index.html", "Decision Analysis: Semi-organized notes", " Decision Analysis: Semi-organized notes Arthur Small SYS 6014 Decision Analysis Spring 2020 Version of: 2020-04-02 This document pulls together class notes from the Spring 2020 edition of SYS 6014 Decision Analysis. These notes are as yet incomplete, and imperfectly organized. They are offered in the hope that they will nonetheless prove useful to students as a review of the theory introduced in class. "],
["intro.html", "1 Introduction 1.1 Motivation: Data-driven decision-making 1.2 The general form of the problems we’ll take up 1.3 Building decision models", " 1 Introduction This course centers on a semester-long project to build a working algorithmic decision tool. As used here, the term algorithmic decision tool means: a computational system that empowers a user to make data-driven, near-optimal (or at least, better) decisions in a specific domain area. In the broadest terms, you can think of a decision tool as having two components, or modules: A prediction component that generates information (possibly imperfect) about the state of the world; and An optimization component that identifies the best action to take, given the information provided by your prediction tool. We introduce the basic concepts of statistical decision theory — a rigorous framework for taking optimal decisions under conditions of uncertainty. In the context of a bare-bones simple model, we address how to formalize a decision problem in terms of uncertain random variables, the menu of possible actions, the potential payoffs from choosing alternative actions, and the decision-maker’s ultimate objectives. We then analyze this model to derive optimal decision rules: rules for choosing actions optimally, given the decision-maker’s beliefs, possible actions, payoffs, and ultimate objectives. We consider how improvements in information can lead to better decisions: with better information, the decision-maker is more likely (though not guaranteed) to opt for actions that improve her results. We also derive formal expressions for the expected value of information, defined as the improvement in the decision-maker’s expected payoffs when the she can make choices on the basis of better information. The expected value of information offers a quantitative measure of the value-add of the decision tool. 1.1 Motivation: Data-driven decision-making Humans make billions of decisions every day. Many of these decisions are not optimal. Many are based on unexamined replication of whatever procedures were used historically. Sub-optimal decisions create waste. Explosion of access to data, analytic tools, &amp; computing power opens new opportunities for decisions to be more data driven. 1.2 The general form of the problems we’ll take up There is a decision-maker who must choose one option from a menu of options. The decision-maker has objectives. These objectives can be quantified in the form of an objective function (equivalently, a loss function). The payoffs to different choices depend on the values of certain state variables or unobserved parameters. The payoff function expresses the costs and benefits the result from the decision-maker’s chosen action. In some applications, it makes sense to model the payoffs as depending on the realized value of a random state variable. In other applications, it makes sense to model the payoff’s as depending on the value of the unobserved parameter. The decision-maker is in general uncertain about the values of the relevant state variables or parameters. In general, we will represent the information the decision-maker has in the form of probability distributions over the relevant state variables or parameters. In most applications we will consider, tools are available to reduce these uncertainties. Depending on the application, the tool might be derived from a statistical model, a machine learning model, or a forecasting system based on a physical model. We will refer to these informational tools collectively as predictive models. 1.3 Building decision models When building a decision model, there are essential questions you must answer early in the process: Who is the decision-maker? What decision does this agent confront? What is the set of options (or potential actions) from which this decision-maker chooses? What are the stakes of the decision? What real costs and benefits are realized from making better vs. poorer decisions? The predictive tool: What information does your chosen predictive tool provide? How will the decision maker use the information generated by this tool to make better decisions? "],
["formalism-for-statistical-decision-models.html", "2 Formalism for statistical decision models 2.1 Actions 2.2 States 2.3 Parameters 2.4 Payoffs", " 2 Formalism for statistical decision models 2.1 Actions A decision-maker must choose from exactly one action \\(a\\) from an action space \\(\\mathbb{A}\\): \\(a \\in \\mathbb{A}\\) 2.1.1 Example The decsion-maker is a doctor. \\(\\mathbb{A} = \\{\\)“give medication”, “don’t give medication”\\(\\}\\). In some applications, each menu option \\(a\\) will correspond to a bundle or vector of distinct actions. In these cases, the action space \\(\\mathbb{A}\\) will include an exhaustive list of all such bundles or vectors. 2.2 States In our nomenclature, states refer to observable data about relevant conditions. A state \\(x\\) will correspond to a single point in a state space. A state space \\(\\mathbb{X}\\) describes the set of all possible values that \\(x\\) might take: \\(x \\in \\mathbb{X}\\). 2.2.1 Example A test can be performed to determine whether a patient has a certain medical condition. Let \\(x\\) denote the test result, which may be positive (\\(x =\\)“Y”) or negative (\\(x =\\)“N”). The state space is the set of all possible outcomes for this test: \\(x \\in \\mathbb{X} = \\{\\)“N”, “Y”\\(\\}\\). Note: Notationally, and computationally, it will often be more convenient to use numeric values for states, e.g., \\(X = \\{0,1\\}\\), where \\(x=1\\) corresponds to a positive test. 2.2.2 Representing uncertain states as random variables Before observing the actual realized value \\(x\\) of the system’s state, the system’s true state is unknown. In a typical application, we will nonetheless have some idea about the likelihood that the system’s state \\(x\\) will attain each of the possible values in the state space \\(\\mathbb{X}\\). Formally, our beliefs about the system’s state can be represented by a probability distribution over the state space \\(\\mathbb{X}\\). 2.2.2.1 Example For \\(x \\in \\mathbb{X} = \\{0,1\\}\\), let \\(f(1) = \\Pr\\{x = 1\\}\\) and \\(f(0) = 1-f(1) = \\Pr\\{x = 0\\}\\). 2.2.3 Random variables Prior to knowing the realized value of the system’s state, its true value is an uncertain random variable. By convention, we will use capital letters to refer to random variables, and lower case letters to refer to corresponding realized values. Here, \\(X\\) denotes a random variable that takes values from on the state space \\(\\mathbb{X}\\), and \\(x\\) denotes its realized value in \\(\\mathbb{X}\\). Let \\(f(x | \\theta)\\) denote the probability that state \\(x\\) will be realized, conditional on the value of the unknown parameter \\(\\theta\\). 2.2.4 Random states For \\(x \\in \\mathbb{X}\\), let \\(f(x)\\) define a probability distribution over \\(\\mathbb{X}\\). Let \\(B \\subset \\mathbb{X}\\). If \\(\\mathbb{X}\\) is discrete, then \\[ \\Pr\\{x \\in B\\} = \\sum_{x \\in B} f(x)\\] If \\(\\mathbb{X}\\) is continuous, then \\[ \\Pr\\{x \\in B\\} = \\int_{x \\in B} f(x)\\] More generally, if \\(x\\) can take any of a finite number of values \\(1, 2, \\ldots, N\\), then a probability distribution over the state space \\(\\mathbb{X} = \\{1, \\ldots, N\\}\\) can be represented by a vector \\(\\theta = &lt;\\theta_1, \\theta_2, \\ldots, \\theta_N&gt;\\), where \\(\\theta_n = \\Pr\\{x = n\\}\\). Since \\(\\theta\\) represents a probability distribution, we must have that \\(\\theta_n \\geq 0\\) for all \\(n\\), and \\(\\sum \\theta_n = 1\\). 2.3 Parameters A parameter is a variable that describes the condition of the system, that is not directly observable. An unobserved parameter \\(\\theta\\) takes values on a parameter space \\(\\mathbb{\\Theta}\\): \\(\\theta \\in \\mathbb{\\Theta}\\). 2.4 Payoffs After choosing an action, the decision-maker realizes a payoff. This payoff depends on the chosen action \\(a\\). Depending on the situation being modeled, it may make sense to express payoffs either in terms of the realized value of the system’s state, or on the value of the unobserved parameters. 2.4.1 Payoffs as a function of action and state Let \\(u(a,x)\\) denote the payoffs from a given action \\(a\\) and state \\(x\\). \\[ E[u(a, X) |\\theta] = \\int_\\mathbb{X} p(x) u(a,x) dx \\] Then define the expected payoff \\(U(a,\\theta)\\) as a function of \\(\\theta\\) in these terms: \\[ U(a,\\theta) = E[u(a, X) |\\theta] = \\int_\\mathbb{X} p(x) u(a,x) dx \\] "],
["statistical-decision-theory.html", "3 Statistical decision theory 3.1 Formalism for statistical decision models", " 3 Statistical decision theory 3.1 Formalism for statistical decision models The payoff to a decision depends on the value of some unobserved parameter \\(\\theta\\), which could be a vector. We have some system for generating a probability distribution \\(\\pi(\\theta)\\) over \\(\\theta\\). In Bayesian approaches, the probability distribution \\(\\pi(\\theta)\\) used is the posterior distribution of \\(\\theta\\) given the observed data: \\(\\pi(\\theta | y)\\). Other approaches exist for creating probability distributions over states "],
["review.html", "4 Review 4.1 Big picture: How to make optimal choices under statistical uncertainty? 4.2 Uncertainty 4.3 Optimal choice 4.4 Integrating predictive and inferential tools into the decision calculus 4.5 Payoff functions: Define in terms of \\(x\\) (state) or \\(\\theta\\) (parameter value)?", " 4 Review 4.1 Big picture: How to make optimal choices under statistical uncertainty? A decision-maker (a.k.a. actor) chooses one option from a menu of possible actions. Payoffs from that choice depend on the action chosen, and on the true value of the state of nature. Payoffs may equivalently be represented as losses — negative payoffs relative to some baseline. The true value of the state of nature is uncertain. Hence the decision-maker confronts a problem of decision-making under uncertainty. Each possible action thus maps to a lottery over uncertain payoffs. 4.2 Uncertainty Typically, the decision-maker will posses some information about the likelihood of different states of nature. This information can be represented formally by treating the state of nature as a random variable drawn from a known distribution. 4.3 Optimal choice Given a complete enumeration of the menu of possible actions, the set of possible states of nature, a probability distribution over the set of states of nature, representing the likelihood of states, and – a payoff function that gives the payoffs (or losses) from each possible combination of action \\(\\times\\) state, in some units of measure. then each possible action maps to a lottery over payoffs (losses). 4.3.1 Optimization: Defining objectives Finally, given a lottery over payoffs, the actor chooses an optimal action guided by a decision-making principle: Example: Maximize expected payoffs. Example: Maximize expected utility of payoffs. Example: ‘Minimax’: Choose the action to minimize possible loss, irrespective of probabilities. The decision-making principle encodes the actor’s attitude towards risk – the willingness to accept losses in some uncertain states of the world, in exchange for acheiving gains in other states of the world. The study of attitudes towards risk and loss is huge topic in economics, finance, and psychology. We will cover it only glancingingly. For your applications, just at minimum be aware that the actor’s optimum choices may not be driven by goal to maximize expected gains (= minimize expected losses). 4.4 Integrating predictive and inferential tools into the decision calculus In general, your predictive model serves to reduce uncertainty over future values of the states of nature. They sharpen the probability distribution over states of nature. (In terms of probability theory: they involve a change of measure.) The actor can now choose an optimal action based not on her prior (or naive) beliefs, but on her posterior beliefs, conditioned on the current data. 4.5 Payoff functions: Define in terms of \\(x\\) (state) or \\(\\theta\\) (parameter value)? Depending on your appplication, it may make sense to model payoffs (or losses) as either a function of observed realized state \\(x \\in \\mathbb{X}\\), e.g., \\(x\\) denotes realized temperature: \\[L_0 = L_0(a;x)\\] or in terms of the value of an unobserved parameter, e.g., \\(\\theta\\) denotes mean temperature: \\[L_1 = L_1(a; \\theta)\\] 4.5.1 Parameter-dependent payoffs as reduced form of state-dependent payoffs In many cases, you can represent the \\(\\theta\\) formulation as the reduced form of the \\(x\\) formulation, e.g. \\[L_1(a;\\theta) = E[L_0(a;x) | \\theta]\\] "],
["prediction.html", "5 Prediction 5.1 Big picture: data-driven decision-making 5.2 Predictive models", " 5 Prediction 5.1 Big picture: data-driven decision-making You have a set of possible actions you can take. You have a prediction model or classification model that enables you to differentiate between cases, based on data. Your decision model then enables you to discriminate — to make the “right” intervention for each different case, rather than just choosing the same action for all cases. 5.2 Predictive models Predictive models may take many different forms. They may use many, many different types of data They may be built using different analytic techniques: statistical regression, machine learning,\\(\\ldots\\) We embrace this diversity of approaches. But: we want our predictions to include uncertainty information. So: want outputs in form of probability distributions over decision-state variables "],
["bayesian-methods.html", "6 Bayesian Methods 6.1 Reminder example: conditional probabilities 6.2 Bayesian methods: Introduction via simple example 6.3 Bayes Theorem 6.4 Sensitivity analysis 6.5 Building a predictive model 6.6 Prediction via the predictive distribution", " 6 Bayesian Methods 6.1 Reminder example: conditional probabilities knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_ch2_social_mobility.png&#39;)) Related readings: Hoff Chs. 1, 3 knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_cover.png&#39;)) 6.2 Bayesian methods: Introduction via simple example Suppose you want to estimate the fraction of a population that is infected with some disease. \\(\\theta \\in [0,1]\\) : true value Test a random sample of \\(20\\) from the population. \\(Y \\in \\{0,1,\\ldots,20\\}\\) : # of positive results. Question: What does realized value of \\(Y\\) tell us about the true value of \\(\\theta\\)? 6.2.1 Sampling model \\(Y | \\theta\\) ~ binomial\\((20,\\theta)\\): For \\(y = 0, 1, \\ldots, 20\\), (i.i.d.) \\[l(y|\\theta) = \\Pr(Y=y | \\theta) = {20 \\choose y} \\theta^y (1-\\theta)^{(20-y)}\\] where \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) \\(l(y|\\theta)\\) called the likelihood function. knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_fig_1-1.png&#39;)) Idea: For any \\(0&lt; \\theta &lt; 1\\), all values of \\(Y\\) are possible, but some are more likely than others. The likelihood function tells us how likely is each possible observation, for a given \\(\\theta\\). If, say, \\(Y = 15\\), that provides evidence that \\(\\theta\\) is not small. Core of Bayesian reasoning: work out all the different combinations of \\(Y, \\theta\\) that could have generated the observed sample data. 6.2.2 Prior information Suppose we have some background knowledge about the likely values of \\(\\theta\\). Represent this knowledge by means of a prior distribution \\(\\pi(\\theta)\\) over \\([0,1]\\). Obviously, there are many (infinitely many) possible such distributions. For convenience, we typically choose to model priors as chosen from a parametrized family of distributions. 6.2.3 The Beta distribution \\[\\theta \\sim \\text{beta}(a,b)\\] Then \\[E[\\theta] = \\frac{a}{a+b}\\] 6.2.4 For our case, let’s suppose our prior beliefs correspond to: \\[\\theta \\sim \\text{beta}(2,20)\\] 6.2.5 \\[\\theta \\sim \\text{beta}(2,20)\\] implies knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_ch1_beta_moments.png&#39;)) 6.3 Bayes Theorem Let \\(\\pi(\\theta | y)\\) denote our posterior distribution over values of \\(\\theta\\). This means: our updated beliefs about the likelihood that \\(\\theta\\) takes various values, after we’ve received our test results. Bayes Theorem says: \\[\\pi(\\theta | y) = \\frac{l(y|\\theta) \\pi(\\theta)}{Pr\\{Y = y\\}} = \\frac{l(y|\\theta) \\pi(\\theta)}{\\int_\\Theta l(y|\\tilde{\\theta})\\pi(\\tilde{\\theta}) d\\tilde{\\theta}}\\] 6.3.1 Can be shown: If \\(\\theta \\sim \\text{beta}(2,20)\\) and \\(Y = 0\\), then \\(\\theta | y \\sim \\text{beta}(2,40)\\). More generally: If \\(\\theta \\sim \\text{beta}(a,b)\\) and \\(Y = y\\), then \\(\\theta | y \\sim \\text{beta}(a+y,b+20-y)\\). 6.3.2 knitr::include_graphics(here(&#39;graphics&#39;,&#39;Exp_beta_posterior.png&#39;)) 6.4 Sensitivity analysis knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_fig_1-2.png&#39;)) 6.5 Building a predictive model 6.5.1 knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_diabetes_model.png&#39;)) 6.5.2 knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_fig_1-3.png&#39;)) 6.5.3 knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_fig_1-4.png&#39;)) 6.6 Prediction via the predictive distribution knitr::include_graphics(here(&#39;graphics&#39;,&#39;Hoff_ch3_predictive_distribution.png&#39;)) "],
["forecast-evaluation.html", "7 Forecast evaluation 7.1 Evaluating forecasting systems: Scoring rules 7.2 Q: “How good is your forecasting system?” 7.3 But is it really?", " 7 Forecast evaluation 7.1 Evaluating forecasting systems: Scoring rules Forecast evaluation involves analyzing how well your forecasting system generates predictions that match observations. Elements of a forecast evaluation: Data: a sequence of matched ordered pairs of forecasts + corresponding observations A scoring rule: A formula that maps the sequence of forecasts and observations to a summary metric of the quality of the forecasting system Important issues: Measurement error in observations: In some cases, verifying observations may not be known with perfect precision. *Scoring rules 7.2 Q: “How good is your forecasting system?” How would you answer? Different ways to measure forecast quality: Correlation between forecasts and verifying observations … 7.3 But is it really? "],
["the-value-of-information.html", "8 The Value of Information", " 8 The Value of Information Idea: Score your forecasting system in terms of how much value it adds to your decision system. "],
["example-decision-problem-whether-to-buy-refrigeration.html", "9 Example decision problem: Whether to buy refrigeration 9.1 Formalism for the refrigeration example 9.2 Statistical forecasting model", " 9 Example decision problem: Whether to buy refrigeration A pub owner in Charlottesville plans to sell beer outside on St. Patrick’s Day, March 17. The pub owner must decide whether to arrange to rent a supplemental refrigeration system for the day. Supplemental refrigeration offers a form of insurance. If temperatures outside on March 17 are high and the pub owner has not arranged supplemental refrigeration, she will be left with warm beer that she will have difficulty selling, leading to financial losses. Conversely, if she pays for supplemental refrigeration when temperatures are low, she will have incurred an unnecessary expense. The analysis here uses statistical decision theory to generate a recommendation about whether or not to pay to rent a supplemental refrigeration system, in order to insure against the potential of financial losses in case of high temperatures. 9.1 Formalism for the refrigeration example 9.1.1 Action set The action set \\(\\mathbb{A}\\) includes just two elements: \\(\\mathbb{A} = \\{a_1, a_2\\}\\), where \\(a_1\\) and \\(a_2\\) correspond to the decsions to not purchase insurance, and to purchase insurance, respectively. 9.1.2 Payoffs: The loss function Let \\(L(a,y)\\) denote the losses incurred by the publican. These depend on the action chosen, and on \\(y\\), the high temperature in Charlotteville on March 17, measured in degrees Celsius. If temperature risks are uninsured (\\(a = a_1\\)), losses increase linearly with temperature for temperatures above a threshold level \\(\\underline{y}\\): if \\(y &gt; \\underline{y}\\), \\[ L(a_1,y) = \\left\\{ \\begin{array}{ll} 0 &amp; y \\leq \\underline{y} \\\\ c_1 (y-\\underline{y}) &amp; y &gt; \\underline{y} \\\\ \\end{array} \\right. \\] for some constant \\(c_1 &gt; 0\\). 9.1.3 Uncertainty Arrangements to arrange supplemental refrigeration must be locked in two weeks in advance, before temperatures are known. Let \\(Y\\) denote the random temperature. 9.1.4 Decision criterion The pub owner is risk-neutral. Her decision criterion is: choose the action that minimizes losses in expectation: \\[ a^* = \\underset{a \\in \\mathbb{A}}{\\operatorname{argmin}}{\\,} E[L(a,Y)]\\] where the expectation is taken over the distiribution of \\(Y\\). This distribution represents the information about uncertain temperatures that the decision-maker has available at the time of she must make her decision. To operationalize this model and derive an optimal solution requires building a system for generating a probabilistic forecast for temperatures on March 17. 9.2 Statistical forecasting model At a two-week time horizon, temperature forecasts based on numerical weather prediction models are not reliable. Instead, a statistical forecasting model is used. 9.2.1 Historical data Let \\(y_1, \\ldots, y_n\\) denote the high temperature in Charlottesville on March 17 for each of the previous \\(n\\) years. 9.2.2 Model of the data generating process It is supposed that these data were generated as independent, identically distributed random draws from a normal distribution: for \\(i = 1, \\ldots, n\\), \\[y_i = \\theta + \\varepsilon_i\\] where \\(\\theta\\) denotes the true but unobserved value of the long-run average temperature, and where \\(\\varepsilon_i \\sim N(0, \\sigma^2_\\varepsilon)\\). 9.2.3 Comments on this statistical model: The risk of model mis-specification This model asserts several substantive assumptions about the data generating process. The process is assumed to be stationary. There is no upward trend over time, no long-term climate change, etc. Temperatures are assumed to be independent from one year to the next. In particular, there is no autocorrelation. Knowing that one year’s temperature was unusually high (say) provides no information about the likelihood that next year’s temperature will also be unusually high. Inter-annual climate cycles (e.g., due to \\(El\\ Ni\\tilde{n}o\\)) are ruled out. Temperature variations around the long-run average are assumed to be identically distributed. This assumption rules out the possibility that variance is, say, greater when temperatures are higher than when they are lower. And others. In general, it is important to formulate a statistical model that accurately reflects the true characteristics of the underlying data generating process. When your statistical model is mis-specified, your probabilistic forecast of future events are likely to stray from the true underlying probabilities. Model mis-specification can then lead to inaccurate estimates of the distribution of losses for each possible action. This error may in turn lead to selection of a sub-optimal action. A particular problem to guard against is the possibility to underestimate the likelihood of extreme events that could cause catastrophic losses. That said, your time on this Earth is limited. Depending on the decision problem and the stakes involved, refining your model to get sharper loss estimates may or may not be worth the bother. A reasonable approach is to start by first writing down a simple forecasting model that appears to capture the essence of the process as you understand it. On the basis of this simple model, generate first-cut probabilistic forecasts of uncertain events. Use these to generate estimated distributions of losses for each possible action in your action set. On that basis, use the specified decision criterion to derive an initial optimal decision rule. Then, go back and check things over more carefully. Review the realism of your statistical model, given your understanding of data generating process. Plot and examine the distribution of your prediction errors. Do your prediction errors appear to follow a pattern that matches what you would expect, given the assumptions you have made? If you find evidence that your prediction model is mis-specified, it may be worth it to go back and refine your model, and see if you generate different results. One very good idea is to perform a sensitivity analysis. How sensitive are your decision recommendations and outcomes to the assumptions you’ve built into your statistical model? If you are not highly confident in your statistical assumptions, and if those assumptions turn out to matter a lot for your recommendations and outcomes, then it could very well be worth the bother to revisit those assumptions, and investigate alternatives. On the other hand, if your decision recommendations are not highly sensitive to your statistical assumptions, then keeping your initial model may be defensible. The point of this work is not to build the best possible prediction system, bullet-proof against any statistical criticism. The point is to help people make good decisions – or at least, decisiions better than they would have made otherwise. Your time and other resources are limited. A good-enough model may be good enough. 9.2.4 Simulation of the prediction process, no covariates set.seed(1) # Keeps random data from changing each time the code is run theta &lt;- 18 # True long-run average temp sigma &lt;- 3 # True stnd dev of temp around this mean n &lt;- 20 # number of simulated historical data points y &lt;- rnorm(n,theta,sigma) hist(y, breaks = 10, main = &quot;Histogram of historical temperatures in degrees C&quot;) boxplot(y) # c0 &lt;- 18 # Threshold temperature for losses # c1 &lt;- 1000 # Loss per degree above threshold, in dollars # # Loss_fcn &lt;- function(temperature){ # for(i in 1:length(temperature)){ # c1*(temperature[i] - c0) # } # } # # losses &lt;- Loss_fcn(y) # # hist(losses, breaks = 10, main = &quot;Empirical distribution of losses based on historically observed temps&quot;) theta_hat &lt;- mean(y) # Sample mean epsilon_hat &lt;- y-theta_hat # Model residuals ssr &lt;- sum(epsilon_hat^2) # Sum of squared residuals sigma_hat &lt;- ssr/(n-1) # Estimated standard error print(theta_hat) ## [1] 18.57157 print(sigma_hat) ## [1] 7.506291 sim_y &lt;- rnorm(10000, theta_hat, sigma_hat) hist(sim_y, breaks = 100) 9.2.5 Using a forecast forecast_bias &lt;- -0.5 forecast_std_error &lt;- 1 forecast_errors &lt;- rnorm(n,mean = forecast_bias,sd = forecast_std_error) x &lt;- y + forecast_errors linear_model &lt;- lm(y~x) plot(x,y, xlab = &quot;Forecast temperature&quot;, ylab = &quot;Observed temperature&quot;) abline(linear_model) plot(x,y) 9.2.6 Simulation of the prediction process, one covariate library(MASS) ## Warning: package &#39;MASS&#39; was built under R version 3.5.2 Sigma &lt;- matrix(c(10,3,3,2),2,2) Sigma ## [,1] [,2] ## [1,] 10 3 ## [2,] 3 2 var(mvrnorm(n=1000, rep(0, 2), Sigma)) ## [,1] [,2] ## [1,] 9.042565 2.692982 ## [2,] 2.692982 1.935439 var(mvrnorm(n=1000, rep(0, 2), Sigma, empirical = TRUE)) ## [,1] [,2] ## [1,] 10 3 ## [2,] 3 2 library(MASS) # Load packages for working with multivariate normal distributions set.seed(1) # Keep random data from changing each time the code is run # Variables # y : temperature on March 17 # x : covariate, e.g., forecast of temp on Mar 17 theta &lt;- 18 x &lt;- rnorm(N, mean=theta, sd=sigma_x) Theta &lt;- c(0,18) # True long-run average Sigma &lt;- 3 # True stnd dev of temp around this mean N &lt;- 40 # number of simulated historical data points y &lt;- rnorm(n,theta,sigma) hist(y, breaks = 10, main = &quot;Histogram of historical temperatures in degrees C&quot;) "],
["the-decision-tool-project.html", "10 The Decision Tool Project 10.1 Assignment 1: Concept Note", " 10 The Decision Tool Project 10.1 Assignment 1: Concept Note In this assignment, you are asked to think about how this framework and these concepts might apply in some real-world application. You are asked to consider some decision problem in which there are real stakes of some kind, and in which a decision-maker could benefit from access to better information. You are asked to describe this decision problem, in outline sketch; and to describe how some information source might help the decision-maker to make better choices. The goal here is not (yet) to articulate a formal model. Rather, the goal is to begin to give some initial thoughts towards selecting your project topic. Relation to your own research: If you are working on a research project that involves analyzing data to extract summary information, you are invited to ask yourself: Who will actually use this information? What will they use it for? How much value does could this information provide? Many people think that if they provide “better” information about some domain area, then better outcomes inevitably will flow forth. But actually, this is not necessarily true. What you are asked to do here is to think rigorously about exactly who will use the information you are providing, how they will use it, and how much value will be realized by empowering them to make better decisions. If you are working hard to create some information tool, but cannot think of any application in which someone might actually use the information output from this tool make better decisions, you might well pause and ask yourself: Why am I doing this? If no one will ever use the information you are generating, then why are you producing it? That question is not meant to be sarcastic, or dismissive. It is meant to press you to think hard about how you are using your scarce time and years on this Earth, and to nudge you in directions that create value. If you are not currently working in a research group to create some predictive or information-productive tool: Think about some work you have done in the past, some predictive tool you are familiar with, or some informative tools used in your area of professional practice. Think about some way this information could be used be a decision-maker to make better decisions. If you are really stuck and can’t think of anything, come see me, and we’ll try to find a good topic together. But first, really try to think of a topic on your own. The assignment: In one page, or at most two, articulate a succinct description of a decision problem along the above lines. Make sure your description addresses the following questions: The decision problem: Who is the decision-maker? What decision does this agent confront? What is the set of options (or potential actions) from which this decision-maker chooses? What are the stakes of the decision? What real costs and benefits are realized from making better vs. poorer decisions? The predictive tool: What information does your chosen predictive tool provide? How will the decision maker use the information generated by this tool to make better decisions? If you are a bit more ambitious, and still have space, you might address some of these questions: In what units are payoffs measured? How, in general terms, are payoffs calculated? What are the uncertain state variables that influence the value of payoffs? What is the range of possible values of these variables? What analytic technique(s) will you use to estimate the values of these uncertain state variables? What data will you use in this estimation? Are you sure you can gain access to these data? In what programming language and platform will you perform your coding? "]
]
